{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5259e8-d7a7-4fe1-b8b3-9577624f759b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Server 3: Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de984847-12c4-4673-8de7-a429206e35a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import pickle\n",
    "import random\n",
    "import tiktoken\n",
    "import py3langid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438a6c9-2aa0-45bc-a097-deb4390e5c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_SIZE = 30\n",
    "api_version = \"2023-12-01-preview\"  # \"2023-05-15\" \n",
    "deploy_name_map = {\"gpt-4-turbo\": \"GPT4-WEST-US\", \"gpt-35-1106\": \"GPT-35-1106\"}\n",
    "official_name_map = {\"gpt-4-turbo\": \"GPT-4 Turbo\", \"gpt-35-1106\": \"GPT-3.5 Turbo\"}\n",
    "client = AzureOpenAI(api_key=\"c6af48fe651d44bb80477d9f17918c3d\", api_version=api_version, azure_endpoint=\"https://gpt-35-1106.openai.azure.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cc9fb-05dd-4c4c-bb7b-52b146ee7eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_fn(y_test, y_pred, average='binary', print_flag=True):\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_test, y_pred, average=average)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_test, y_pred, average=average)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_test, y_pred, average=average)*100, 2)\n",
    "    if print_flag:\n",
    "        print(\"Accuracy:\", acc, \"% | Precision:\", precision, \"% | Recall:\", recall, \"% | F1:\", f1, \"%\\n\")\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "\n",
    "def compare_baseline(exp_df_ls, exp_model_ls, exp_method_ls):\n",
    "    ls_res = []\n",
    "    for i, df_tmp in enumerate(exp_df_ls):\n",
    "        ls_res.append((exp_method_ls[i], exp_model_ls[i])+eval_fn(df_tmp[\"same\"], df_tmp[\"answer\"], print_flag=False) + df_tmp.shape)\n",
    "    res = pd.DataFrame(ls_res, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Size', 'df.shape[1]'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def num_tokens_from_string(texts, model_id):\n",
    "    encoding = tiktoken.encoding_for_model(model_id)\n",
    "    num_tokens = len(encoding.encode(texts))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4942e14-d5ca-41f8-895c-51c1115512f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prep (Blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84c96e-76bc-4607-9419-adcc5c747ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data1/baixiang/dataset/blogtext.csv\")\n",
    "df.drop(['gender', 'age', 'topic', 'sign', 'date'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a564e-6629-46d6-8169-3b24e6b4e433",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "df[df[['text']].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6c74e-cee2-40d7-9b4e-fdd114bf0311",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('# duplicates:', df.text.duplicated().sum(), 'sanity check:', df.shape[0] - len(set(df.text)))\n",
    "print('Before removing duplicates, df.shape:', df.shape)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d20c7-c515-44ee-a811-94a3f8efb658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = df.id.value_counts()\n",
    "df = df[df.id.isin(v[v >= 2].index)]\n",
    "print('# unique authors', len(df.id.unique()))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa384e6-baee-4df3-b3ee-e578ea1246e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"{df.shape[0]:,}\")\n",
    "df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n",
    "print('% of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n",
    "\n",
    "df = df[df.lang=='en']\n",
    "df.drop('lang', axis=1, inplace=True)\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf39168-c0ab-473f-b154-6654e928b9d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check # of tokens\n",
    "for i in range(10):\n",
    "    text1, text2 = df.sample(2).text.values\n",
    "    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d444cc7-88d5-422c-a717-66418366563e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prep (Mail)\n",
    "https://www.kaggle.com/datasets/wcukierski/enron-email-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9066c-d859-4b6d-8ff0-743fad95021b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv(\"/data1/baixiang/dataset/enron-emails.csv\")\n",
    "emails_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8739c-74ca-4bd9-a689-67b04839c1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import email\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append(part.get_payload())\n",
    "    return ''.join(parts)\n",
    "\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs\n",
    "\n",
    "\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df['message'])) \n",
    "for key in messages[0].keys():\n",
    "    emails_df[key] = [doc[key] for doc in messages]\n",
    "emails_df['Text'] = list(map(get_text_from_email, messages))\n",
    "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
    "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
    "del messages\n",
    "emails_df = emails_df[['From', 'To', 'Text', 'Date', 'message']]\n",
    "emails_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128dbf1-8379-4155-ab78-ace1d9829012",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in emails_df.index:\n",
    "    sender = emails_df.loc[i, 'From']\n",
    "    receiver = emails_df.loc[i, 'To']\n",
    "    if type(sender) is list and len(sender) > 1:\n",
    "        print('More than 1 sender:', sender)\n",
    "    \n",
    "    # if receiver is None:\n",
    "    #     receiver = 'nan'\n",
    "    # # elif len(emails_df.loc[i, 'To']) > 1:\n",
    "    # #     print('More than 1 receiver:', emails_df.loc[i, 'To'])\n",
    "    \n",
    "emails_df['From'] = emails_df[\"From\"].apply(lambda x: list(x)[0])\n",
    "# emails_df['To'] = emails_df[\"To\"].apply(lambda x: ' '.join(list(x)))#.astype(\"unicode\")\n",
    "emails_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33808d08-f96e-48b2-ac0c-1b9ad232f846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "emails_df[emails_df[['Text']].duplicated(keep=False)].sort_values('Text').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde7be89-02f4-4acd-89d1-25efddf8fd0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# emails_df = emails_df.drop_duplicates(subset=['From', 'To', 'Text', 'Date'], keep='first').reset_index(drop=True)\n",
    "emails_df = emails_df.drop_duplicates(subset=['Text'], keep='first').reset_index(drop=True)\n",
    "emails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d77fb2-94a2-4ede-9ea9-6f43ae3ad261",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mail_corpus = emails_df.copy()\n",
    "mail_corpus.columns = ['user', 'receiver', 'text', 'date', 'message_old']\n",
    "\n",
    "unique_author = mail_corpus['user'].unique()\n",
    "email_mapping = {k: v for k, v in zip(unique_author, range(len(unique_author)))}\n",
    "mail_corpus['id'] = mail_corpus['user'].apply(lambda x: 'mail_'+str(email_mapping[x]))\n",
    "mail_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ce1c0-09cb-4edf-ba88-95240ecda6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = mail_corpus\n",
    "df.drop(['user', 'receiver', 'date', 'message_old'], axis=1, inplace=True)\n",
    "print(df[df['text']==''].shape)\n",
    "df.text = df.text.str.strip()\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be12f8a7-6e3c-45b6-8c2f-cf24d233d7db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{df.shape[0]:,}\") \n",
    "print(df[df['text']==''].shape)\n",
    "df.text = df.text.str.strip()\n",
    "df.dropna(inplace=True)\n",
    "print(f\"{df.shape[0]:,}\") \n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2046c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "df[df[['text']].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7fc27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('# duplicates:', df.text.duplicated().sum(), 'sanity check:', df.shape[0] - len(set(df.text)))\n",
    "print('Before removing duplicates, df.shape:', df.shape)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85872d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v = df.id.value_counts()\n",
    "df = df[df.id.isin(v[v >= 2].index)]\n",
    "print('# unique authors', len(df.id.unique()))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe231b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"{df.shape[0]:,}\")\n",
    "df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n",
    "print('% of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n",
    "\n",
    "df = df[df.lang=='en']\n",
    "df.drop('lang', axis=1, inplace=True)\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036b45a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check # of tokens\n",
    "for i in range(10):\n",
    "    text1, text2 = df.sample(2).text.values\n",
    "    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72d9bf7-81fe-4c6c-9873-3d6bdca7a6f3",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898de8b-0ef4-4766-bbfa-6e4704c783d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampler_fn(df, size=DATA_SIZE):\n",
    "    \"\"\"Sample a subset in a balanced way\"\"\"\n",
    "    dict_to_df = []\n",
    "    text_set = set()\n",
    "    author_ls = random.sample(df.id.unique().tolist(), size*2)\n",
    "\n",
    "    for i in range(size):\n",
    "        if i % 2 == 0:  # sample documents from different authors\n",
    "            aut_id1, aut_id2 = random.sample(author_ls, 2)\n",
    "            text1 = df[df.id==aut_id1].text.sample(1).values[0]\n",
    "            text2 = df[df.id==aut_id2].text.sample(1).values[0]\n",
    "            author_ls.remove(aut_id1)\n",
    "            author_ls.remove(aut_id2)\n",
    "        else:  # sample documents from same authors to make it balance\n",
    "            same_auth_id = random.choice(author_ls)\n",
    "            author_ls.remove(same_auth_id)\n",
    "            aut_id1, aut_id2 = same_auth_id, same_auth_id\n",
    "            text1, text2 = df[df.id==same_auth_id].sample(2).text.tolist()\n",
    "            while text1 in text_set or text2 in text_set:\n",
    "                text1, text2 = df[df.id==same_auth_id].sample(2).text.tolist()\n",
    "        # print(text1, text2)\n",
    "        dict_row = {}\n",
    "        dict_row[\"text1\"], dict_row[\"text2\"] = text1, text2\n",
    "        dict_row[\"aut_id1\"], dict_row[\"aut_id2\"] = aut_id1, aut_id2\n",
    "        text_set.add(text1)\n",
    "        text_set.add(text2)\n",
    "        dict_to_df.append(dict_row)\n",
    "\n",
    "    df_sub = pd.DataFrame(dict_to_df)\n",
    "    df_sub['same'] = df_sub.aut_id1 == df_sub.aut_id2\n",
    "    print('# same authors:', df_sub['same'].sum(), '# different authors:', len(np.unique(df_sub.aut_id1)))\n",
    "    return df_sub\n",
    "    \n",
    "\n",
    "# df_sub = sampler_fn(df)\n",
    "df_sub = pd.read_csv(\"llm-verify-res/df_sub_blog_30.csv\")\n",
    "print(df_sub.shape)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ff8210-b810-481f-889a-ebe2d2291b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_sub[df_sub[['text1']].duplicated(keep=False)].shape)\n",
    "print(df_sub[df_sub[['text2']].duplicated(keep=False)].shape)\n",
    "print(df_sub[df_sub[['aut_id1']].duplicated(keep=False)].shape)\n",
    "print(df_sub[df_sub[['aut_id2']].duplicated(keep=False)].shape)\n",
    "# Avg number of words\n",
    "(df_sub['text1'].apply(lambda x: len(x.split())).mean() + df_sub['text2'].apply(lambda x: len(x.split())).mean()) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575dc5ae-3c9e-4684-a79a-295e4a7ca924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.aut_id1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fe2602-812f-45a9-a04c-71a7795dd3e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    text1, text2 = df_sub.loc[i, 'text1'], df_sub.loc[i, 'text2']\n",
    "    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c723424-09fb-4627-86b6-2a3c423ab65a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_verfication(df, method, model_name, prompt_prefix, system_msg, ls_df, ls_model, ls_method, prompt_postfix=\"\"):\n",
    "    ls = []\n",
    "    start_time = time.time()\n",
    "    print(\"\\n++++++++++ \", method, model_name, \" ++++++++++\")\n",
    "    \n",
    "    for i in df.index:\n",
    "        aut_id1, aut_id2 = df.loc[i, 'aut_id1'], df.loc[i, 'aut_id2']\n",
    "        text1, text2 = df.loc[i, 'text1'], df.loc[i, 'text2']\n",
    "        prompt = prompt_prefix + f\"\"\"The input texts (Text 1 and Text 2) are delimited with triple backticks. ```\\n\\nText 1: {text1}, \\n\\nText 2: {text2}\\n\\n```\"\"\" + prompt_postfix\n",
    "        \n",
    "        raw_response = client.chat.completions.create(\n",
    "            model=deploy_name_map[model_name], \n",
    "            response_format={\"type\": \"json_object\"} if model_name in [\"gpt-35-1106\", \"gpt-4-turbo\"] else None, \n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_msg},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ], \n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        response_str = raw_response.choices[0].message.content\n",
    "        print('Raw response content:', response_str, '\\n')\n",
    "        try:\n",
    "            response = json.loads(response_str, strict=False)  \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"===== JSONDecodeError =====\\n\")\n",
    "            response = json.loads(\"{}\")\n",
    "            response['answer'] = not(aut_id1 == aut_id2)  # generate a wrong answer when JSONDecodeError occur\n",
    "            response['analysis'] = 'JSONDecodeError' + response_str\n",
    "            # continue\n",
    "        \n",
    "        response[\"text1\"], response[\"text2\"] = text1, text2\n",
    "        response[\"author_id1\"], response[\"author_id2\"] = aut_id1, aut_id2\n",
    "        response[\"tokens\"] = raw_response.usage.total_tokens\n",
    "        ls.append(response)\n",
    "        response = None\n",
    "    df_res = pd.DataFrame(ls)\n",
    "    ls_df.append(df_res)\n",
    "    ls_method.append(method)\n",
    "    ls_model.append(official_name_map[model_name])\n",
    "    df_res['same'] = df_res.author_id1 == df_res.author_id2\n",
    "    df_res[\"answer\"] = df_res[\"answer\"].astype('bool')\n",
    "    eval_fn(df_res[\"same\"], df_res[\"answer\"])\n",
    "    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8094c4-c950-4543-9dda-edc76f1bdb93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083defed-48ca-4365-86a9-19f3ab48a8af",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d071f4-7142-41da-9d0f-0016253919a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"\n",
    "Respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"analysis\": Reasoning behind your answer.\n",
    "  \"answer\":  A boolean (True/False) answer.\n",
    "}\n",
    "\"\"\"\n",
    "prompt1 = \"\"\"\n",
    "Verify if two input texts were written by the same author.\n",
    "\"\"\"\n",
    "prompt2 = \"\"\"\n",
    "Verify if two input texts were written by the same author. Analyze the writing styles of the input texts, disregarding the differences in topic and content.\n",
    "\"\"\"\n",
    "prompt3 = \"\"\"\n",
    "Verify if two input texts were written by the same author. Focus on grammatical styles indicative of authorship.\n",
    "\"\"\"\n",
    "prompt4 = \"\"\"\n",
    "Verify if two input texts were written by the same author. Analyze the writing styles of the input texts, disregarding the differences in topic and content. Reasoning based on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d445834-57af-435c-9682-a28d24c02ca4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_df_1, ls_model_1, ls_method_1 = [], [], []\n",
    "\n",
    "df1_gpt35 = run_verfication(df_sub, v1, 'gpt-35-1106', prompt1, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "df1_gpt4 = run_verfication(df_sub, v1, 'gpt-4-turbo', prompt1, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "\n",
    "df2_gpt35 = run_verfication(df_sub, v2, 'gpt-35-1106', prompt2, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "df2_gpt4 = run_verfication(df_sub, v2, 'gpt-4-turbo', prompt2, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "\n",
    "df3_gpt35 = run_verfication(df_sub, v3, 'gpt-35-1106', prompt3, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "df3_gpt4 = run_verfication(df_sub, v3, 'gpt-4-turbo', prompt3, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "\n",
    "df4_gpt35 = run_verfication(df_sub, v4, 'gpt-35-1106', prompt4, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "df4_gpt4 = run_verfication(df_sub, v4, 'gpt-4-turbo', prompt4, system_msg, ls_df_1, ls_model_1, ls_method_1)\n",
    "\n",
    "res1 = compare_baseline(ls_df_1, ls_model_1, ls_method_1)\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d5373-cf92-4086-bf4e-dfa82e24ccda",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_df_2, ls_model_2, ls_method_2 = [], [], []\n",
    "\n",
    "df1_gpt35 = run_verfication(df_sub, v1, 'gpt-35-1106', prompt1, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "df1_gpt4 = run_verfication(df_sub, v1, 'gpt-4-turbo', prompt1, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "\n",
    "df2_gpt35 = run_verfication(df_sub, v2, 'gpt-35-1106', prompt2, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "df2_gpt4 = run_verfication(df_sub, v2, 'gpt-4-turbo', prompt2, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "\n",
    "df3_gpt35 = run_verfication(df_sub, v3, 'gpt-35-1106', prompt3, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "df3_gpt4 = run_verfication(df_sub, v3, 'gpt-4-turbo', prompt3, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "\n",
    "df4_gpt35 = run_verfication(df_sub, v4, 'gpt-35-1106', prompt4, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "df4_gpt4 = run_verfication(df_sub, v4, 'gpt-4-turbo', prompt4, system_msg, ls_df_2, ls_model_2, ls_method_2)\n",
    "\n",
    "res2 = compare_baseline(ls_df_2, ls_model_2, ls_method_2)\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e7e2b-048d-4bfb-814f-52ce04221c5b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_df_3, ls_model_3, ls_method_3 = [], [], []\n",
    "\n",
    "df1_gpt35 = run_verfication(df_sub, v1, 'gpt-35-1106', prompt1, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "df1_gpt4 = run_verfication(df_sub, v1, 'gpt-4-turbo', prompt1, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "\n",
    "df2_gpt35 = run_verfication(df_sub, v2, 'gpt-35-1106', prompt2, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "df2_gpt4 = run_verfication(df_sub, v2, 'gpt-4-turbo', prompt2, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "\n",
    "df3_gpt35 = run_verfication(df_sub, v3, 'gpt-35-1106', prompt3, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "df3_gpt4 = run_verfication(df_sub, v3, 'gpt-4-turbo', prompt3, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "\n",
    "df4_gpt35 = run_verfication(df_sub, v4, 'gpt-35-1106', prompt4, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "df4_gpt4 = run_verfication(df_sub, v4, 'gpt-4-turbo', prompt4, system_msg, ls_df_3, ls_model_3, ls_method_3)\n",
    "\n",
    "res3 = compare_baseline(ls_df_3, ls_model_3, ls_method_3)\n",
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f9ba3-1d70-4e63-90ea-0cb2074b8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = res1.drop(['Size', 'df.shape[1]'], axis=1)\n",
    "r2 = res2.drop(['Size', 'df.shape[1]'], axis=1)\n",
    "r3 = res3.drop(['Size', 'df.shape[1]'], axis=1)\n",
    "res_con = pd.concat([r1, r2, r3])\n",
    "\n",
    "res_mean = res_con.groupby(['Method', 'Model'], as_index=False, sort=False).mean().round(decimals=2)\n",
    "res_std = res_con.groupby(['Method', 'Model'], as_index=False, sort=False).std().round(decimals=2)\n",
    "res_max = res_con.groupby(['Method', 'Model'], as_index=False, sort=False).max().round(decimals=2)\n",
    "res_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f42433-bd7c-4ac5-978f-48ab0e236762",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mean.astype(str).iloc[:, 2:]+'±'+res_std.astype(str).iloc[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa52e59-9895-41a9-a9bc-a6a99e1bb606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
