{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb94b3ba-f271-424b-8c38-9fe4a2e8546e",
   "metadata": {},
   "source": [
    "## llama 2 70B\n",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart.html\n",
    "\n",
    "https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321f8d1-a439-445a-b4d1-f5f6b0510589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from huggingface_hub import login\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "login(\"replace_this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad209075-26c3-479b-920e-35bca26791fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_id = \"TheBloke/Llama-2-70B-Chat-AWQ\"  # AWQ takes 10 more minutes and doesn't improve performance\n",
    "# llm = LLM(model=\"TheBloke/Llama-2-13b-Chat-AWQ\", quantization=\"AWQ\"\")\n",
    "\n",
    "model_id = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "llm = LLM(model=model_id, quantization='gptq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5445eca-b701-4103-8b7f-c732e26b12f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_fn(y_test, y_pred, average='binary', print_flag=True):\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_test, y_pred, average=average)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_test, y_pred, average=average)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_test, y_pred, average=average)*100, 2)\n",
    "    if print_flag:\n",
    "        print(\"Accuracy:\", acc, \"% | Precision:\", precision, \"% | Recall:\", recall, \"% | F1:\", f1, \"%\\n\")   \n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "\n",
    "def compare_baseline(exp_df_ls, exp_model_ls, exp_method_ls):\n",
    "    ls_res = []\n",
    "    for i, df_tmp in enumerate(exp_df_ls):\n",
    "        ls_res.append((exp_method_ls[i], exp_model_ls[i])+eval_fn(df_tmp[\"same\"], df_tmp[\"answer\"], print_flag=False) + df_tmp.shape)\n",
    "    res = pd.DataFrame(ls_res, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Size', 'df.shape[1]'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def num_tokens_from_string(texts, model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    encoding = tokenizer(texts)\n",
    "    num_tokens = len(encoding.input_ids)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0343fe8-cafd-48e2-ad9a-7b3c576f50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_verfication(df, method, model_name, prompt_prefix, system_msg, ls_df, ls_model, ls_method, prompt_postfix=\"\", print_flag=False):\n",
    "    ls = []\n",
    "    start_time = time.time()\n",
    "    print(\"\\n++++++++++ \", method, model_name, \" ++++++++++\")\n",
    "    \n",
    "    for i in df.index:\n",
    "        aut_id1, aut_id2 = df.loc[i, 'aut_id1'], df.loc[i, 'aut_id2']\n",
    "        text1, text2 = df.loc[i, 'text1'].strip(), df.loc[i, 'text2'].strip()\n",
    "        # prompt = prompt_prefix + f\"\"\"The input texts (Text 1 and Text 2) are delimited with triple backticks. ```Text 1: {text1}, \\n\\nText 2: {text2}```\\n\\n\"\"\" + prompt_postfix\n",
    "        prompt = f\"\"\"<s>[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{prompt_prefix} The input texts (Text 1 and Text 2) are delimited with triple backticks. ```Text 1: {text1}, \\n\\nText 2: {text2}```\\n\\n{prompt_postfix}[/INST]\"\"\"\n",
    "        # print('\\nprompt + Input texts:', prompt)\n",
    "\n",
    "        raw_response = llm.generate(prompt, sampling_params)\n",
    "        response_str = raw_response[0].outputs[0].text.strip()\n",
    "        \n",
    "        response = json.loads(\"{}\")\n",
    "        response['analysis'] = response_str\n",
    "        if 'True' in response_str or 'true' in response_str:\n",
    "            response['answer'] = True\n",
    "        elif 'False' in response_str or 'false' in response_str:\n",
    "            response['answer'] = False\n",
    "        else:\n",
    "            response['answer'] = not(aut_id1 == aut_id2)  # generate a wrong answer\n",
    "        if print_flag:\n",
    "            print('Response:\\n', response['analysis'], '\\nLabel:', aut_id1 == aut_id2)\n",
    "        \n",
    "        response[\"text1\"], response[\"text2\"] = text1, text2\n",
    "        response[\"author_id1\"], response[\"author_id2\"] = aut_id1, aut_id2\n",
    "        response[\"tokens\"] = len(raw_response[0].prompt_token_ids)  # Number of input tokens\n",
    "        ls.append(response)\n",
    "        response = None\n",
    "    df_res = pd.DataFrame(ls)\n",
    "    ls_df.append(df_res)\n",
    "    ls_method.append(method)\n",
    "    ls_model.append(model_name)\n",
    "    df_res['same'] = df_res.author_id1 == df_res.author_id2\n",
    "    df_res[\"answer\"] = df_res[\"answer\"].astype('bool')\n",
    "    eval_fn(df_res[\"same\"], df_res[\"answer\"])\n",
    "    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e00c89-1ee5-4c71-87db-f0ef0bde5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=4096)\n",
    "sampling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f514aa-77cd-4cac-934e-c390e0c5e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ff527-98fa-45a1-8ea9-d3b943bc385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"\"\"\n",
    "Respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"analysis\": Reasoning behind your answer.\n",
    "  \"answer\":  A boolean (True/False) answer.\n",
    "}\n",
    "\"\"\"\n",
    "prompt1 = \"Verify if the input texts were written by the same author. \"\n",
    "prompt2 = prompt1 + \"Do not consider topic differences. \"\n",
    "prompt3 = prompt1 + \"Focus on grammatical styles. \"\n",
    "prompt4 = prompt1 + \"Analyze the writing styles of the input texts, disregarding the differences in topic and content. Reasoning based on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb3278a-33c2-4681-9c1e-788bf7fe357e",
   "metadata": {},
   "source": [
    "### Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149493e0-ec3c-473a-89c4-c3ed40a80e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_blog = pd.read_csv(\"llm-verify-res/df_sub_blog_30.csv\")\n",
    "print(df_sub_blog.shape)\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a78cf-7311-4d6b-b570-cdec2bfa6561",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_llama_blog_1, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, 'llama2-70b', prompt1, system_msg, ls_llama_blog_1, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, 'llama2-70b', prompt2, system_msg, ls_llama_blog_1, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, 'llama2-70b', prompt3, system_msg, ls_llama_blog_1, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, 'llama2-70b', prompt4, system_msg, ls_llama_blog_1, ls_model, ls_method)\n",
    "compare_baseline(ls_llama_blog_1, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002d6c9-9465-4b62-8e0c-ca0d8fa87fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_llama_blog_2, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, 'llama2-70b', prompt1, system_msg, ls_llama_blog_2, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, 'llama2-70b', prompt2, system_msg, ls_llama_blog_2, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, 'llama2-70b', prompt3, system_msg, ls_llama_blog_2, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, 'llama2-70b', prompt4, system_msg, ls_llama_blog_2, ls_model, ls_method)\n",
    "compare_baseline(ls_llama_blog_2, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d036e1-a701-459c-81cc-0593213138e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_llama_blog_3, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, 'llama2-70b', prompt1, system_msg, ls_llama_blog_3, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, 'llama2-70b', prompt2, system_msg, ls_llama_blog_3, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, 'llama2-70b', prompt3, system_msg, ls_llama_blog_3, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, 'llama2-70b', prompt4, system_msg, ls_llama_blog_3, ls_model, ls_method)\n",
    "compare_baseline(ls_llama_blog_3, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a914cd-bda1-47b4-98ef-5447ee4c8b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm-verify-res/llama_70b_blog_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_llama_blog_1, f)\n",
    "with open(\"llm-verify-res/llama_70b_blog_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_llama_blog_2, f)\n",
    "with open(\"llm-verify-res/llama_70b_blog_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_llama_blog_3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f6efa0-523f-4459-a019-5e7d1c9176e6",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2800e326-f796-41d6-ace5-3eb4fc1252b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_email = pd.read_csv(\"llm-verify-res/df_sub_email_30.csv\")\n",
    "print(df_sub_email.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1741f01-5f87-4aa9-b777-17ec955bae5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_email_1, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_email, v1, 'llama2-70b', prompt1, system_msg, ls_email_1, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_email, v2, 'llama2-70b', prompt2, system_msg, ls_email_1, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_email, v3, 'llama2-70b', prompt3, system_msg, ls_email_1, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_email, v4, 'llama2-70b', prompt4, system_msg, ls_email_1, ls_model, ls_method)\n",
    "compare_baseline(ls_email_1, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d61b3-0b7c-427a-b664-7dd2d7b18cde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_email_2, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_email, v1, 'llama2-70b', prompt1, system_msg, ls_email_2, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_email, v2, 'llama2-70b', prompt2, system_msg, ls_email_2, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_email, v3, 'llama2-70b', prompt3, system_msg, ls_email_2, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_email, v4, 'llama2-70b', prompt4, system_msg, ls_email_2, ls_model, ls_method)\n",
    "compare_baseline(ls_email_2, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866b368-0dfa-4d7a-ad96-a9b90541b526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_email_3, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_email, v1, 'llama2-70b', prompt1, system_msg, ls_email_3, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_email, v2, 'llama2-70b', prompt2, system_msg, ls_email_3, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_email, v3, 'llama2-70b', prompt3, system_msg, ls_email_3, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_email, v4, 'llama2-70b', prompt4, system_msg, ls_email_3, ls_model, ls_method)\n",
    "compare_baseline(ls_email_3, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7066ddc2-1cb9-4e6e-80cf-70286298ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm-verify-res/llama_70b_email_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_email_1, f)\n",
    "with open(\"llm-verify-res/llama_70b_email_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_email_2, f)\n",
    "with open(\"llm-verify-res/llama_70b_email_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_email_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf08a52-bb04-4257-995b-01252b90dc77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
