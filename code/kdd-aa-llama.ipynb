{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5259e8-d7a7-4fe1-b8b3-9577624f759b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Server 4: Authorship Attribution\n",
    "- New prompts\n",
    "- Chunked data\n",
    "- New sampler (Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n",
    "    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de984847-12c4-4673-8de7-a429206e35a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "import random\n",
    "import tiktoken\n",
    "import py3langid\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import shuffle\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc837a34-8ce5-41be-8953-68c68c7991a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from huggingface_hub import login\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "login(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dae4bf-5b8c-4c36-8f27-1639fe4c227e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "llm = LLM(model=model_id, quantization='gptq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cc9fb-05dd-4c4c-bb7b-52b146ee7eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name):\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "\n",
    "def eval_fn(y_test, y_pred, average='weighted', print_flag=True):\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_test, y_pred, average=average)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_test, y_pred, average=average, zero_division=0)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_test, y_pred, average=average, zero_division=0)*100, 2)\n",
    "    if print_flag:\n",
    "        print(\"Accuracy:\", acc, \"% | Precision:\", precision, \"% | Recall:\", recall, \"% | F1:\", f1, \"%\\n\")   \n",
    "    return acc, precision, recall, f1\n",
    "    \n",
    "\n",
    "def embed_fn(model_name, texts, baseline_type):\n",
    "    if baseline_type == 'bert':\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenized_texts = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        embedding = model(tokenized_texts.input_ids.to(model.device), tokenized_texts.attention_mask.to(model.device)).last_hidden_state.mean(dim=1)\n",
    "    elif baseline_type == 'tf-idf':\n",
    "        vectorizer = TfidfVectorizer(max_features=3000, analyzer='char', ngram_range=(4, 4))\n",
    "        embedding = torch.from_numpy(vectorizer.fit_transform(texts).toarray())\n",
    "    elif baseline_type == 'ada':\n",
    "        ada_client = AzureOpenAI(api_key = \"08e99b6c65e84ead8676c505ee4d6f1e\", api_version = \"2023-05-15\", azure_endpoint = \"https://iarpa.openai.azure.com\")\n",
    "        ada_response = ada_client.embeddings.create(input = texts, model = \"test_embedding\")\n",
    "        embedding = torch.Tensor([e.embedding for e in ada_response.data])\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def run_aa_baseline(df_sub, model_name, baseline_type='bert', print_flag=True, eval_average='weighted'):\n",
    "    ls_acc, ls_precision, ls_recall, ls_f1 = [], [], [], []\n",
    "\n",
    "    for i in df_sub.index:\n",
    "        ls_query_text, ls_potential_text = df_sub.loc[i, 'query_text'], df_sub.loc[i, 'potential_text']\n",
    "        embed_query_texts = F.normalize(embed_fn(model_name, ls_query_text, baseline_type)) \n",
    "        embed_potential_texts = F.normalize(embed_fn(model_name, ls_potential_text, baseline_type))\n",
    "        \n",
    "        preds = embed_query_texts @ embed_potential_texts.T\n",
    "        preds = F.softmax(preds, dim=-1)\n",
    "        labels = np.arange(0, len(ls_query_text))\n",
    "\n",
    "        acc, precision, recall, f1 = eval_fn(labels, preds.argmax(-1).numpy(), eval_average, print_flag)\n",
    "        ls_acc.append(acc)\n",
    "        ls_precision.append(precision)\n",
    "        ls_recall.append(recall)\n",
    "        ls_f1.append(f1)\n",
    "\n",
    "    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_precision), 2), round(np.mean(ls_recall), 2), round(np.mean(ls_f1), 2))\n",
    "    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_precision), 2), round(np.std(ls_recall), 2), round(np.std(ls_f1), 2))\n",
    "    return muti_avg, muti_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4942e14-d5ca-41f8-895c-51c1115512f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84c96e-76bc-4607-9419-adcc5c747ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/baixiang/dataset/blogtext.csv\")\n",
    "df.drop(['gender', 'age', 'topic', 'sign', 'date'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a564e-6629-46d6-8169-3b24e6b4e433",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "df[df[['text']].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe76f-8c3d-40d1-af0c-1ab8a46aa3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Before removing duplicates, df.shape:', df.shape)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa384e6-baee-4df3-b3ee-e578ea1246e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"{df.shape[0]:,}\")\n",
    "df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n",
    "print('% of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n",
    "\n",
    "df = df[df.lang=='en']\n",
    "df.drop('lang', axis=1, inplace=True)\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d46c91-e956-4df0-96ef-60704421f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check # of tokens\n",
    "for i in range(10):\n",
    "    text1, text2 = df.sample(2).text.values\n",
    "    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05110a2-8b6a-4e2e-8fc1-6781dc48060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") < 512)]\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7e109-b290-4187-8ad5-41a8e0573dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") > 56)]\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5e164-e8b6-47f4-bd5d-b5b668bf7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df.id.value_counts()\n",
    "df = df[df.id.isin(v[v >= 2].index)]\n",
    "print('# unique authors:', df.id.nunique())\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898de8b-0ef4-4766-bbfa-6e4704c783d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampler_aa_fn_pro(df, n, reps):\n",
    "    \"\"\"\n",
    "    Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n",
    "    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.\n",
    "    All the authors are unique as long as n is less than the number of unique authors.\n",
    "    n: number of candidate authors.\n",
    "    reps: number of repetitions.\n",
    "    \"\"\"\n",
    "    dict_to_df = []\n",
    "    ls_unique_author = df.id.unique().tolist()\n",
    "    for _ in range(reps):\n",
    "        candidate_authors = random.sample(ls_unique_author, n)\n",
    "        ls_unique_author = [e for e in ls_unique_author if e not in candidate_authors]\n",
    "        ls_queries, ls_potential_texts = [], []\n",
    "        dict_row = {}\n",
    "        \n",
    "        for author_id in candidate_authors:\n",
    "            # each text in these 2 lists are from unique authors, texts at same index are from the same author\n",
    "            text, text_same_author = df.loc[author_id == df.id].text.sample(2)\n",
    "            ls_queries.append(text)\n",
    "            ls_potential_texts.append(text_same_author)\n",
    "\n",
    "        dict_row[\"query_text\"] = ls_queries\n",
    "        dict_row[\"potential_text\"] = ls_potential_texts\n",
    "        dict_to_df.append(dict_row)\n",
    "\n",
    "    df_sub = pd.DataFrame(dict_to_df)\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692dbe0-63a8-4e5e-b6be-c417d09387ca",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5455f60-92e5-453c-80af-454f277d4914",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def run_aa(df, method, model_name, prompt_input, system_msg, ls_df, ls_model, ls_method, n_eval=5):\n",
    "    \"\"\"randomly select a subset of query texts\"\"\"\n",
    "    start_time = time.time()\n",
    "    df_res_all = pd.DataFrame()\n",
    "    print(\"\\n++++++++++ \", method, model_name, \" ++++++++++\")\n",
    "\n",
    "    for i in df.index:\n",
    "        ls_reps = []\n",
    "        text_label_map = {}\n",
    "        sampled_queries = []  # select a subset for evaluation (e.g, n_eval out of 10)\n",
    "        ls_query_text, ls_potential_text = df.loc[i, 'query_text'], df.loc[i, 'potential_text']\n",
    "        random.seed(0)\n",
    "        for idx, val in random.sample(list(enumerate(ls_query_text)), n_eval):\n",
    "            text_label_map[val] = idx\n",
    "            sampled_queries.append(val)\n",
    "        # print(text_label_map.values())\n",
    "            \n",
    "        for query_text in sampled_queries:\n",
    "            example_texts = json.dumps(dict(enumerate(ls_potential_text)))\n",
    "            # mistral \n",
    "            # prompt = f\"\"\"<s> [INST] {system_msg} {prompt_input} The input texts are delimited with triple backticks. ```\\n\\nQuery text: {query_text} \\n\\nTexts from potential authors: {example_texts}\\n\\n```[/INST]\"\"\"\n",
    "            # llama\n",
    "            prompt = f\"\"\"<s>[INST] <<SYS>>\\n{system_msg}\\n<</SYS>>\\n\\n{prompt_input} The input texts are delimited with triple backticks. ```\\n\\nQuery text: {query_text} \\n\\nTexts from potential authors: {example_texts}\\n\\n```[/INST]\"\"\"\n",
    "       \n",
    "            raw_response = llm.generate(prompt, sampling_params)\n",
    "            response_str = raw_response[0].outputs[0].text.strip()\n",
    "        \n",
    "            response = json.loads(\"{}\")\n",
    "            response['analysis'] = response_str\n",
    "            # ls_possible_ans = [s for s in response_str.split() if s.isdigit()]\n",
    "            ls_possible_ans = re.findall(r'\\d+', response_str)\n",
    "            if len(ls_possible_ans) > 0:\n",
    "                response['answer'] = ls_possible_ans[-1]\n",
    "            else:\n",
    "                response['answer'] = -1\n",
    "            print('\\n++++++++++ Raw response:\\n', response['analysis'], '\\nModel prediction:', response['answer'], 'Label:', text_label_map[query_text])\n",
    "                \n",
    "            response[\"query_text\"], response[\"example_texts\"] = query_text, example_texts\n",
    "            response[\"tokens\"] = len(raw_response[0].prompt_token_ids)\n",
    "            response[\"label\"] = text_label_map[query_text]\n",
    "            ls_reps.append(response)\n",
    "\n",
    "        df_reps = pd.DataFrame(ls_reps)\n",
    "        df_reps['answer'] = pd.to_numeric(df_reps['answer'], errors='coerce')\n",
    "        df_reps['answer'] = df_reps['answer'].fillna(-1)\n",
    "        df_res_all = pd.concat([df_res_all, df_reps]).reset_index(drop=True)\n",
    "\n",
    "    ls_df.append(df_res_all)\n",
    "    ls_method.append(method)\n",
    "    ls_model.append(model_name)\n",
    "    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    return df_res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e823f2-cbb1-4d50-9d7d-4523e9e22ab4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_baseline = {'TF-IDF':'TF-IDF', 'BERT':'bert-base-uncased', \n",
    "                 'RoBERTa':'roberta-base', 'ELECTRA':'google/electra-base-discriminator',\n",
    "                 'DeBERTa':'microsoft/deberta-base', 'Ada':'ada v2'}\n",
    "dict_embed_type = {'TF-IDF':'tf-idf', 'BERT':'bert', 'RoBERTa':'bert', \n",
    "                   'ELECTRA':'bert', 'DeBERTa':'bert', 'Ada':'ada'}\n",
    "\n",
    "def compare_baseline_mod(df_sub, ls_df, ls_model, ls_method, n_eval=5, std_flag=False):\n",
    "    ls_res_avg, ls_res_std = [], []\n",
    "\n",
    "    for key, val in list(dict_baseline.items())[:0]:\n",
    "        muti_avg, muti_std = run_aa_baseline(df_sub, val, dict_embed_type[key], print_flag=False)\n",
    "        ls_res_avg.append((key, val)+muti_avg+(0,))\n",
    "        ls_res_std.append((key, val)+muti_std+(0,))\n",
    "\n",
    "    for i, df_tmp in enumerate(ls_df):\n",
    "        muti_avg, muti_std = eval_all_fn(df_tmp, n_eval)\n",
    "        answer_tmp = df_tmp.copy()\n",
    "        \n",
    "        ls_res_avg.append((ls_method[i], ls_model[i])+muti_avg+(abs(answer_tmp[answer_tmp.answer==-1]['answer'].astype('int').sum()),))\n",
    "        ls_res_std.append((ls_method[i], ls_model[i])+muti_std+(None,))\n",
    "    \n",
    "    res_avg = pd.DataFrame(ls_res_avg, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Unsure'])\n",
    "    res_std = pd.DataFrame(ls_res_std, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Unsure'])\n",
    "    if std_flag:\n",
    "        return res_avg, res_std\n",
    "    else:\n",
    "        return res_avg\n",
    "\n",
    "\n",
    "def eval_all_fn(df_res_all, n_eval):\n",
    "    \"\"\"evaluate the entire df of multiple repetitions, take avg of each rep. \n",
    "    The null or -1 answers are counted as false\n",
    "    Make sure n_eval is same in run_aa()\"\"\"\n",
    "    ls_acc, ls_precision, ls_recall, ls_f1 = [], [], [], []\n",
    "    for i in range(0, len(df_res_all.index), n_eval):\n",
    "        df_reps = df_res_all[i: i+n_eval]\n",
    "        acc, precision, recall, f1 = eval_fn(df_reps[\"label\"], df_reps[\"answer\"], average='weighted', print_flag=False)\n",
    "        ls_acc.append(acc)\n",
    "        ls_precision.append(precision)\n",
    "        ls_recall.append(recall)\n",
    "        ls_f1.append(f1)\n",
    "        \n",
    "    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_precision), 2), round(np.mean(ls_recall), 2), round(np.mean(ls_f1), 2))\n",
    "    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_precision), 2), round(np.std(ls_recall), 2), round(np.std(ls_f1), 2))\n",
    "    return muti_avg, muti_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df22c17-e817-42c9-95d9-53fc58e5fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=4096)\n",
    "sampling_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083defed-48ca-4365-86a9-19f3ab48a8af",
   "metadata": {},
   "source": [
    "## n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3509683-d403-4748-8a68-9bce5ec8d5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1, m2 = \"GPT-3.5 Turbo\", \"GPT-4 Turbo\"\n",
    "v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'\n",
    "\n",
    "prompt1 = \"Given a set of texts with known authors and a query text, determine the author of the query text. \"\n",
    "prompt2 = prompt1+\"Do not consider topic differences. \"\n",
    "prompt3 = prompt1+\"Focus on grammatical styles. \"\n",
    "prompt4 = prompt1+\"Analyze the writing styles of the input texts, disregarding the differences in topic and content. Focus on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \"\n",
    "system_msg = \"\"\"Always respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"analysis\": Reasoning behind your answer.\n",
    "  \"answer\": The query text's author ID.\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b68892-f27f-4188-b1a5-89228c2adcca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "df_10 = pd.read_csv(\"llm-aa-res/blog_n10_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "df_10.shape, len(df_10.loc[0, 'potential_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feaf09e-22d6-4896-b2b9-ab169cff9598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_df_10, ls_model_10, ls_method_10 = [], [], []\n",
    "df1 = run_aa(df_10, v1, model_id, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df2 = run_aa(df_10, v2, model_id, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df3 = run_aa(df_10, v3, model_id, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df4 = run_aa(df_10, v4, model_id, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "compare_baseline_mod(df_10, ls_df_10, ls_model_10, ls_method_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747cfb9c-0d91-44af-a262-4fd59ce1992c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_10 = pd.read_csv(\"llm-aa-res/email_n10_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "print(df_10.shape, len(df_10.loc[0, 'potential_text']))\n",
    "\n",
    "ls_df_10, ls_model_10, ls_method_10 = [], [], []\n",
    "df1 = run_aa(df_10, v1, model_id, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df2 = run_aa(df_10, v2, model_id, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df3 = run_aa(df_10, v3, model_id, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df4 = run_aa(df_10, v4, model_id, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "compare_baseline_mod(df_10, ls_df_10, ls_model_10, ls_method_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a159b387-df21-4b2d-ae63-90afd9303619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79373d32-38ed-4fa5-8dae-4489eccf969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
