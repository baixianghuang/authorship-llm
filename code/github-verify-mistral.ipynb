{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb94b3ba-f271-424b-8c38-9fe4a2e8546e",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321f8d1-a439-445a-b4d1-f5f6b0510589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from huggingface_hub import login\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\n",
    "login(\"replace_this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad209075-26c3-479b-920e-35bca26791fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = LLM(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5445eca-b701-4103-8b7f-c732e26b12f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_fn(y_test, y_pred, average='binary', print_flag=True):\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_test, y_pred, average=average)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_test, y_pred, average=average)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_test, y_pred, average=average)*100, 2)\n",
    "    if print_flag:\n",
    "        print(\"Accuracy:\", acc, \"% | Precision:\", precision, \"% | Recall:\", recall, \"% | F1:\", f1, \"%\\n\")   \n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "\n",
    "def compare_baseline(exp_df_ls, exp_model_ls, exp_method_ls):\n",
    "    ls_res = []\n",
    "    for i, df_tmp in enumerate(exp_df_ls):\n",
    "        ls_res.append((exp_method_ls[i], exp_model_ls[i])+eval_fn(df_tmp[\"same\"], df_tmp[\"answer\"], print_flag=False) + df_tmp.shape)\n",
    "    res = pd.DataFrame(ls_res, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Size', 'df.shape[1]'])\n",
    "    return res\n",
    "\n",
    "\n",
    "def num_tokens_from_string(texts, model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    encoding = tokenizer(texts)\n",
    "    num_tokens = len(encoding.input_ids)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271647f-d61f-4858-9b1c-4981d8f97719",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": system_msg},\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"{prompt1} The input texts (Text 1 and Text 2) are delimited with triple backticks. ```Text 1:, \\n\\nText 2: ```\\n\\n\"\"\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0959c76-69fb-4e87-b5b0-e4214bffb181",
   "metadata": {},
   "source": [
    "## Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68211e50-1595-4cbd-9642-a47dd64cb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_verfication(df, method, model_name, prompt_prefix, system_msg, ls_df, ls_model, ls_method, prompt_postfix=\"\", print_flag=False):\n",
    "    ls = []\n",
    "    start_time = time.time()\n",
    "    print(\"\\n++++++++++ \", method, model_name, \" ++++++++++\")\n",
    "    \n",
    "    for i in df.index:\n",
    "        aut_id1, aut_id2 = df.loc[i, 'aut_id1'], df.loc[i, 'aut_id2']\n",
    "        text1, text2 = df.loc[i, 'text1'].strip(), df.loc[i, 'text2'].strip()\n",
    "        # prompt = prompt_prefix + f\"\"\"The input texts (Text 1 and Text 2) are delimited with triple backticks. ```Text 1: {text1}, \\n\\nText 2: {text2}```\\n\\n\"\"\" + prompt_postfix\n",
    "        prompt = f\"\"\"<s> [INST] {system_msg} {prompt_prefix} The input texts (Text 1 and Text 2) are delimited with triple backticks. ```Text 1: {text1}, \\n\\nText 2: {text2}```[/INST]\"\"\"\n",
    "        # print('\\nprompt + Input texts:', prompt)\n",
    "          \n",
    "        raw_response = llm.generate(prompt, sampling_params)\n",
    "        response_str = raw_response[0].outputs[0].text.strip()\n",
    "        \n",
    "        response_json = json.loads(\"{}\")\n",
    "        response_json['analysis'] = response_str\n",
    "        if 'true' in response_str.lower():\n",
    "            response_json['answer'] = True\n",
    "        elif 'false' in response_str.lower():\n",
    "            response_json['answer'] = False\n",
    "        else:\n",
    "            response_json['answer'] = not(aut_id1 == aut_id2)  # generate a wrong answer\n",
    "        if print_flag:\n",
    "            print('Response:\\n', response_json['analysis'], '\\nLabel:', aut_id1 == aut_id2)\n",
    "        \n",
    "        response_json[\"text1\"], response_json[\"text2\"] = text1, text2\n",
    "        response_json[\"author_id1\"], response_json[\"author_id2\"] = aut_id1, aut_id2\n",
    "        response_json[\"tokens\"] = len(raw_response[0].prompt_token_ids)  # Number of input tokens\n",
    "        ls.append(response_json)\n",
    "        response_json = None\n",
    "    df_res = pd.DataFrame(ls)\n",
    "    ls_df.append(df_res)\n",
    "    ls_method.append(method)\n",
    "    ls_model.append(model_name)\n",
    "    df_res['same'] = df_res.author_id1 == df_res.author_id2\n",
    "    df_res[\"answer\"] = df_res[\"answer\"].astype('bool')\n",
    "    eval_fn(df_res[\"same\"], df_res[\"answer\"])\n",
    "    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    print(sampling_params)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f514aa-77cd-4cac-934e-c390e0c5e701",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'\n",
    "system_msg = \"\"\"\n",
    "Respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"analysis\": Reasoning behind your answer.\n",
    "  \"answer\":  A boolean (True/False) answer.\n",
    "}\n",
    "\"\"\"\n",
    "prompt1 = \"Verify if the input texts were written by the same author. \"\n",
    "prompt2 = prompt1 + \"Do not consider topic differences. \"\n",
    "prompt3 = prompt1 + \"Focus on grammatical styles. \"\n",
    "prompt4 = prompt1 + \"Analyze the writing styles of the input texts, disregarding the differences in topic and content. Reasoning based on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c682e41-5375-4629-bffc-7c0a2293c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=4096)\n",
    "sampling_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526c77e-279d-4ced-93d7-4a4dc8cf23ad",
   "metadata": {},
   "source": [
    "### Blog ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96553df-64f9-494f-b684-8e69dc45402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_blog = pd.read_csv(\"llm-verify-res/df_sub_blog_30.csv\")\n",
    "print(df_sub_blog.shape)\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a7f4e-6d8c-45fa-921d-b355baf06350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_blog_1, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, model_id, prompt1, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, model_id, prompt2, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, model_id, prompt3, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, model_id, prompt4, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "compare_baseline(ls_blog_1, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2230bed-6ac8-48ca-92c1-61d369a4d94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_blog_2, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, model_id, prompt1, system_msg, ls_blog_2, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, model_id, prompt2, system_msg, ls_blog_2, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, model_id, prompt3, system_msg, ls_blog_2, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, model_id, prompt4, system_msg, ls_blog_2, ls_model, ls_method)\n",
    "compare_baseline(ls_blog_2, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f52aaa-f42c-45ce-98ec-bc4be9131547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_blog_3, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, model_id, prompt1, system_msg, ls_blog_3, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, model_id, prompt2, system_msg, ls_blog_3, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, model_id, prompt3, system_msg, ls_blog_3, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, model_id, prompt4, system_msg, ls_blog_3, ls_model, ls_method)\n",
    "compare_baseline(ls_blog_3, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7650fb-e42f-4f49-ad47-61caeece7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm-verify-res/mistral_7b_blog_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_blog_1, f)\n",
    "with open(\"llm-verify-res/mistral_7b_blog_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_blog_2, f)\n",
    "with open(\"llm-verify-res/mistral_7b_blog_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_blog_3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4877e10e-72b3-4edf-837a-1362ed2fda3a",
   "metadata": {},
   "source": [
    "### Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e42d1fc-fb75-4f83-b70e-6492fd1dcea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_email = pd.read_csv(\"llm-verify-res/df_sub_email_30.csv\")\n",
    "print(df_sub_email.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439db0e-44de-4eac-a7b9-d0e791a14524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_email_1, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_email, v1, model_id, prompt1, system_msg, ls_email_1, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_email, v2, model_id, prompt2, system_msg, ls_email_1, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_email, v3, model_id, prompt3, system_msg, ls_email_1, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_email, v4, model_id, prompt4, system_msg, ls_email_1, ls_model, ls_method)\n",
    "compare_baseline(ls_email_1, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f76212d-fcb8-4bd2-9197-35ee9efa6f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_email_2, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_email, v1, model_id, prompt1, system_msg, ls_email_2, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_email, v2, model_id, prompt2, system_msg, ls_email_2, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_email, v3, model_id, prompt3, system_msg, ls_email_2, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_email, v4, model_id, prompt4, system_msg, ls_email_2, ls_model, ls_method)\n",
    "compare_baseline(ls_email_2, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d4f7c-3fd0-45fc-825c-b23e4df8cfd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_email_3, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_email, v1, model_id, prompt1, system_msg, ls_email_3, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_email, v2, model_id, prompt2, system_msg, ls_email_3, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_email, v3, model_id, prompt3, system_msg, ls_email_3, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_email, v4, model_id, prompt4, system_msg, ls_email_3, ls_model, ls_method)\n",
    "compare_baseline(ls_email_3, ls_model, ls_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faa6f63-be97-4814-9a41-61ad9be9700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm-verify-res/mistral_7b_email_1.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_email_1, f)\n",
    "with open(\"llm-verify-res/mistral_7b_email_2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_email_2, f)\n",
    "with open(\"llm-verify-res/mistral_7b_email_3.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_email_3, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee25e3b-fa0d-4934-8cf1-319f95f356a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_blog = pd.read_csv(\"llm-verify-res/df_sub_blog_30.csv\")\n",
    "print(df_sub_blog.shape)\n",
    "print(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3337fdd-bb85-42de-8327-0dc9ef064135",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_blog_1, ls_model, ls_method = [], [], []\n",
    "df1 = run_verfication(df_sub_blog, v1, model_id, prompt1, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "df2 = run_verfication(df_sub_blog, v2, model_id, prompt2, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "df3 = run_verfication(df_sub_blog, v3, model_id, prompt3, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "df4 = run_verfication(df_sub_blog, v4, model_id, prompt4, system_msg, ls_blog_1, ls_model, ls_method)\n",
    "compare_baseline(ls_blog_1, ls_model, ls_method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
