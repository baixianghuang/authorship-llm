{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5259e8-d7a7-4fe1-b8b3-9577624f759b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Authorship Attribution using GPT-3.5 and GPT-4 (n_eval=10, reps=3)\n",
    "- Evaluation metrics: acc, f1_w, f1_micro, f1_macro\n",
    "- In each rep, randomly choose a subset of size n_eval for evaluation\n",
    "- sampler (Sample a new list of authors every time, use each of author as a query author so that the number of labels = n. Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de984847-12c4-4673-8de7-a429206e35a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import openai\n",
    "import pickle\n",
    "import random\n",
    "import tiktoken\n",
    "import py3langid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import shuffle\n",
    "from sklearn import metrics\n",
    "from ast import literal_eval\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d64cc-3ecb-40cd-84ad-9948bc49efaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EVAL=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cc9fb-05dd-4c4c-bb7b-52b146ee7eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name):\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "\n",
    "def eval_fn(y_test, y_pred):\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n",
    "    f1_w = round(metrics.f1_score(y_test, y_pred, average='weighted')*100, 2)\n",
    "    f1_micro = round(metrics.f1_score(y_test, y_pred, average='micro')*100, 2)\n",
    "    f1_macro = round(metrics.f1_score(y_test, y_pred, average='macro')*100, 2)\n",
    "    return acc, f1_w, f1_micro, f1_macro\n",
    "    \n",
    "\n",
    "def embed_fn(model_name, texts, baseline_type):\n",
    "    if baseline_type == 'bert':\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenized_texts = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        embedding = model(tokenized_texts.input_ids.to(model.device), tokenized_texts.attention_mask.to(model.device)).last_hidden_state.mean(dim=1)\n",
    "    elif baseline_type == 'tf-idf':\n",
    "        vectorizer = TfidfVectorizer(max_features=3000, analyzer='char', ngram_range=(4, 4))\n",
    "        embedding = torch.from_numpy(vectorizer.fit_transform(texts).toarray())\n",
    "    elif baseline_type == 'ada':\n",
    "        ada_client = AzureOpenAI(api_key = \"replace_this\", api_version = \"2023-05-15\", azure_endpoint = \"replace_this\")\n",
    "        ada_response = ada_client.embeddings.create(input = texts, model = \"replace_this\")\n",
    "        embedding = torch.Tensor([e.embedding for e in ada_response.data])\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def run_aa_baseline(df_sub, model_name, baseline_type='bert'):\n",
    "    ls_acc, ls_f1_w, ls_f1_micro, ls_f1_macro = [], [], [], []\n",
    "\n",
    "    for i in df_sub.index:\n",
    "        ls_query_text, ls_potential_text = df_sub.loc[i, 'query_text'], df_sub.loc[i, 'potential_text']\n",
    "        embed_query_texts = F.normalize(embed_fn(model_name, ls_query_text, baseline_type)) \n",
    "        embed_potential_texts = F.normalize(embed_fn(model_name, ls_potential_text, baseline_type))\n",
    "        \n",
    "        preds = embed_query_texts @ embed_potential_texts.T\n",
    "        preds = F.softmax(preds, dim=-1)\n",
    "        labels = np.arange(0, len(ls_query_text))\n",
    "\n",
    "        acc, f1_w, f1_micro, f1_macro = eval_fn(labels, preds.argmax(-1).numpy())\n",
    "        ls_acc.append(acc)\n",
    "        ls_f1_w.append(f1_w)\n",
    "        ls_f1_micro.append(f1_micro)\n",
    "        ls_f1_macro.append(f1_macro)\n",
    "\n",
    "    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_f1_w), 2), round(np.mean(ls_f1_micro), 2), round(np.mean(ls_f1_macro), 2))\n",
    "    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_f1_w), 2), round(np.std(ls_f1_micro), 2), round(np.std(ls_f1_macro), 2))\n",
    "    return muti_avg, muti_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4942e14-d5ca-41f8-895c-51c1115512f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data prep for the Blog dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba84c96e-76bc-4607-9419-adcc5c747ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"blogtext.csv\")\n",
    "df.drop(['gender', 'age', 'topic', 'sign', 'date'], axis=1, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a564e-6629-46d6-8169-3b24e6b4e433",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "df[df[['text']].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fe76f-8c3d-40d1-af0c-1ab8a46aa3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Before removing duplicates, df.shape:', df.shape)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa384e6-baee-4df3-b3ee-e578ea1246e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"{df.shape[0]:,}\")\n",
    "df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n",
    "print('% of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n",
    "\n",
    "df = df[df.lang=='en']\n",
    "df.drop('lang', axis=1, inplace=True)\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d46c91-e956-4df0-96ef-60704421f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check # of tokens\n",
    "for i in range(10):\n",
    "    text1, text2 = df.sample(2).text.values\n",
    "    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05110a2-8b6a-4e2e-8fc1-6781dc48060c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") < 512)]\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e7e109-b290-4187-8ad5-41a8e0573dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") > 56)]\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5e164-e8b6-47f4-bd5d-b5b668bf7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df.id.value_counts()\n",
    "df = df[df.id.isin(v[v >= 2].index)]\n",
    "print('# unique authors:', df.id.nunique())\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898de8b-0ef4-4766-bbfa-6e4704c783d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampler_aa_fn_pro(df, n, reps):\n",
    "    \"\"\"\n",
    "    Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n",
    "    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.\n",
    "    All the authors are unique as long as n is less than the number of unique authors.\n",
    "    n: number of candidate authors.\n",
    "    reps: number of repetitions.\n",
    "    \"\"\"\n",
    "    dict_to_df = []\n",
    "    ls_unique_author = df.id.unique().tolist()\n",
    "    for _ in range(reps):\n",
    "        candidate_authors = random.sample(ls_unique_author, n)\n",
    "        ls_unique_author = [e for e in ls_unique_author if e not in candidate_authors]\n",
    "        ls_queries, ls_potential_texts = [], []\n",
    "        dict_row = {}\n",
    "        \n",
    "        for author_id in candidate_authors:\n",
    "            # each text in these 2 lists are from unique authors, texts at same index are from the same author\n",
    "            text, text_same_author = df.loc[author_id == df.id].text.sample(2)\n",
    "            ls_queries.append(text)\n",
    "            ls_potential_texts.append(text_same_author)\n",
    "\n",
    "        dict_row[\"query_text\"] = ls_queries\n",
    "        dict_row[\"potential_text\"] = ls_potential_texts\n",
    "        dict_to_df.append(dict_row)\n",
    "\n",
    "    df_sub = pd.DataFrame(dict_to_df)\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501f606-4b48-4c7a-bdb4-cc56650588c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data prep for the Enron Email dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64be0c2f-7864-47c5-8011-abe351fffedd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emails_df = pd.read_csv(\"enron-emails.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf96aeb-9873-47c1-ae46-0e774c7ab281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import email\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append(part.get_payload())\n",
    "    return ''.join(parts)\n",
    "\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = frozenset(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs\n",
    "\n",
    "\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, emails_df['message'])) \n",
    "for key in messages[0].keys():\n",
    "    emails_df[key] = [doc[key] for doc in messages]\n",
    "emails_df['Text'] = list(map(get_text_from_email, messages))\n",
    "emails_df['From'] = emails_df['From'].map(split_email_addresses)\n",
    "emails_df['To'] = emails_df['To'].map(split_email_addresses)\n",
    "del messages\n",
    "emails_df = emails_df[['From', 'To', 'Text', 'Date', 'message']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4a7a5-c4da-4527-8f8a-10d337af8e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in emails_df.index:\n",
    "    sender = emails_df.loc[i, 'From']\n",
    "    receiver = emails_df.loc[i, 'To']\n",
    "    if type(sender) is list and len(sender) > 1:\n",
    "        print('More than 1 sender:', sender)\n",
    "    \n",
    "emails_df['From'] = emails_df[\"From\"].apply(lambda x: list(x)[0])\n",
    "emails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f3a7f-f59b-4684-89b5-11810493032b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "emails_df[emails_df[['Text']].duplicated(keep=False)].sort_values('Text').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993db30-3852-407a-86b5-252c02031392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emails_df = emails_df.drop_duplicates(subset=['Text'], keep='first').reset_index(drop=True)\n",
    "emails_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051e29f-4395-4211-9d12-cf19527027a9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mail_corpus = emails_df.copy()\n",
    "mail_corpus.columns = ['user', 'receiver', 'text', 'date', 'message_old']\n",
    "unique_author = mail_corpus['user'].unique()\n",
    "email_mapping = {k: v for k, v in zip(unique_author, range(len(unique_author)))}\n",
    "mail_corpus['id'] = mail_corpus['user'].apply(lambda x: 'mail_'+str(email_mapping[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30431396-e56e-4867-a8a0-4451972ba07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(mail_corpus[mail_corpus['text']==''].shape)\n",
    "mail_corpus.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0837e-9aaa-4c7d-bfd1-03d7dd741dee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mail_corpus.text = mail_corpus.text.apply(lambda x: x.strip())\n",
    "mail_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2edc8c1-5f38-452e-8be2-4f96d79fec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "mail_corpus.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2386213a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = mail_corpus[['text', 'id']].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c71d2e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding and removing duplicate rows\n",
    "df[df[['text']].duplicated(keep=False)].sort_values('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93004e89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Before removing duplicates, df.shape:', df.shape)\n",
    "df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba754d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(f\"{df.shape[0]:,}\")\n",
    "df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n",
    "print('% of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n",
    "\n",
    "df = df[df.lang=='en']\n",
    "df.drop('lang', axis=1, inplace=True)\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41dcb4-e7be-421a-a753-894a6f676cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") < 512)]\n",
    "# print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923815b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") > 64)]\n",
    "print(f\"{df.shape[0]:,}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4625004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = df.id.value_counts()\n",
    "df = df[df.id.isin(v[v >= 2].index)]\n",
    "print('# unique authors:', df.id.nunique())\n",
    "print('New df.shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0c15f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sampler_aa_fn_pro(df, n, reps):\n",
    "    \"\"\"\n",
    "    Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n",
    "    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.\n",
    "    All the authors are unique as long as n is less than the number of unique authors\n",
    "    n: number of candidate authors.\n",
    "    reps: number of repetitions.\n",
    "    \"\"\"\n",
    "    dict_to_df = []\n",
    "    ls_unique_author = df.id.unique().tolist()\n",
    "    for _ in range(reps):\n",
    "        candidate_authors = random.sample(ls_unique_author, n)\n",
    "        ls_unique_author = [e for e in ls_unique_author if e not in candidate_authors]\n",
    "        ls_queries, ls_potential_texts = [], []\n",
    "        dict_row = {}\n",
    "        \n",
    "        for author_id in candidate_authors:\n",
    "            # each text in these 2 lists are from unique authors, texts at same index are from the same author (makes the testing data imbalanced?)\n",
    "            text, text_same_author = df.loc[author_id == df.id].text.sample(2)\n",
    "            ls_queries.append(text)\n",
    "            ls_potential_texts.append(text_same_author)\n",
    "\n",
    "        dict_row[\"query_text\"] = ls_queries\n",
    "        dict_row[\"potential_text\"] = ls_potential_texts\n",
    "        dict_to_df.append(dict_row)\n",
    "\n",
    "    df_sub = pd.DataFrame(dict_to_df)\n",
    "    return df_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b17925-4ee8-46cf-bd56-5fd83d38392d",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5455f60-92e5-453c-80af-454f277d4914",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_aa(df, method, model_name, prompt_input, system_msg, ls_df, ls_model, ls_method, n_eval=N_EVAL):\n",
    "    \"\"\"randomly select a subset of query texts\"\"\"\n",
    "    start_time = time.time()\n",
    "    df_res_all = pd.DataFrame()\n",
    "    print(\"\\n++++++++++ \", method, model_name, n_eval, \" ++++++++++\")\n",
    "\n",
    "    for i in df.index:\n",
    "        ls_reps = []\n",
    "        text_label_map = {}\n",
    "        sampled_queries = []  # select a subset for evaluation (e.g, n_eval out of 10)\n",
    "        ls_query_text, ls_potential_text = df.loc[i, 'query_text'], df.loc[i, 'potential_text']\n",
    "        random.seed(0)\n",
    "        for idx, val in random.sample(list(enumerate(ls_query_text)), n_eval):\n",
    "            text_label_map[val] = idx\n",
    "            sampled_queries.append(val)\n",
    "            \n",
    "        for query_text in sampled_queries:\n",
    "            example_texts = json.dumps(dict(enumerate(ls_potential_text)))\n",
    "            prompt = prompt_input+f\"\"\"The input texts are delimited with triple backticks. ```\\n\\nQuery text: {query_text} \\n\\nTexts from potential authors: {example_texts}\\n\\n```\"\"\"\n",
    "            # List of potential author IDs: {list(dict(enumerate(ls_potential_text)).keys())}\n",
    "                \n",
    "            raw_response = client.chat.completions.create(\n",
    "                model=model_name, \n",
    "                response_format={\"type\": \"json_object\"}, \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_msg},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ], \n",
    "                temperature=0\n",
    "            )\n",
    "\n",
    "            response_str = raw_response.choices[0].message.content\n",
    "            print('\\nRaw response content:\\n', response_str, '\\nLabel:', text_label_map[query_text])\n",
    "            try: \n",
    "                response = json.loads(response_str, strict=False)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"++++++++++ JSONDecodeError ++++++++++\")\n",
    "                response = json.loads(\"{}\")\n",
    "                response['analysis'] = response_str\n",
    "                response['answer'] = -1\n",
    "\n",
    "            response[\"query_text\"], response[\"example_texts\"] = query_text, example_texts\n",
    "            response[\"tokens\"] = raw_response.usage.total_tokens\n",
    "            response[\"label\"] = text_label_map[query_text]\n",
    "            ls_reps.append(response)\n",
    "            response = None\n",
    "\n",
    "        df_reps = pd.DataFrame(ls_reps)\n",
    "        df_reps['answer'] = pd.to_numeric(df_reps['answer'], errors='coerce')\n",
    "        df_reps['answer'] = df_reps['answer'].fillna(-1)\n",
    "        df_res_all = pd.concat([df_res_all, df_reps]).reset_index(drop=True)\n",
    "\n",
    "    ls_df.append(df_res_all)\n",
    "    ls_method.append(method)\n",
    "    ls_model.append(model_name)\n",
    "    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    return df_res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e823f2-cbb1-4d50-9d7d-4523e9e22ab4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_baseline = {'TF-IDF':'TF-IDF', 'BERT':'bert-base-uncased', \n",
    "                 'RoBERTa':'roberta-base', 'ELECTRA':'google/electra-base-discriminator',\n",
    "                 'DeBERTa':'microsoft/deberta-base'}\n",
    "dict_embed_type = {'TF-IDF':'tf-idf', 'BERT':'bert', 'RoBERTa':'bert', \n",
    "                   'ELECTRA':'bert', 'DeBERTa':'bert'}\n",
    "\n",
    "def compare_baseline_mod(df_sub, ls_df, ls_model, ls_method, n_eval=N_EVAL, std_flag=False, baseline_idx=len(dict_baseline)):\n",
    "    ls_res_avg, ls_res_std = [], []\n",
    "\n",
    "    for key, val in list(dict_baseline.items())[:baseline_idx]:\n",
    "        muti_avg, muti_std = run_aa_baseline(df_sub, val, dict_embed_type[key])\n",
    "        ls_res_avg.append((key, val)+muti_avg+(0,))\n",
    "        ls_res_std.append((key, val)+muti_std+(0,))\n",
    "\n",
    "    for i, df_tmp in enumerate(ls_df):\n",
    "        muti_avg, muti_std = eval_all_fn(df_tmp, n_eval)\n",
    "        answer_tmp = df_tmp.copy()\n",
    "        \n",
    "        ls_res_avg.append((ls_method[i], ls_model[i])+muti_avg+(abs(answer_tmp[answer_tmp.answer==-1]['answer'].astype('int').sum()),))\n",
    "        ls_res_std.append((ls_method[i], ls_model[i])+muti_std+(None,))\n",
    "    \n",
    "    res_avg = pd.DataFrame(ls_res_avg, columns=ls_col)\n",
    "    res_std = pd.DataFrame(ls_res_std, columns=ls_col)\n",
    "    if std_flag:\n",
    "        return res_avg, res_std\n",
    "    else:\n",
    "        return res_avg\n",
    "\n",
    "\n",
    "def eval_all_fn(df_res_all, n_eval):\n",
    "    \"\"\"evaluate the entire df of multiple repetitions, take avg of each rep. \n",
    "    The null or -1 answers are counted as false\n",
    "    Make sure n_eval is same in run_aa()\"\"\"\n",
    "    ls_acc, ls_f1_w, ls_f1_micro, ls_f1_macro = [], [], [], []\n",
    "    for i in range(0, len(df_res_all.index), n_eval):\n",
    "        df_reps = df_res_all[i: i+n_eval]\n",
    "        acc, f1_w, f1_micro, f1_macro = eval_fn(df_reps[\"label\"], df_reps[\"answer\"])\n",
    "        ls_acc.append(acc)\n",
    "        ls_f1_w.append(f1_w)\n",
    "        ls_f1_micro.append(f1_micro)\n",
    "        ls_f1_macro.append(f1_macro)\n",
    "\n",
    "    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_f1_w), 2), round(np.mean(ls_f1_micro), 2), round(np.mean(ls_f1_macro), 2))\n",
    "    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_f1_w), 2), round(np.std(ls_f1_micro), 2), round(np.std(ls_f1_macro), 2))\n",
    "    return muti_avg, muti_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a438a6c9-2aa0-45bc-a097-deb4390e5c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_version = \"2023-12-01-preview\"  # \"2023-05-15\" \n",
    "client = AzureOpenAI(api_key=\"replace_this\", api_version=api_version, azure_endpoint=\"replace_this\")\n",
    "\n",
    "ls_col = ['Prompt', 'Model', 'Accuracy', 'Weighted F1', 'Micro F1', 'Macro F1', 'Unsure']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083defed-48ca-4365-86a9-19f3ab48a8af",
   "metadata": {},
   "source": [
    "## n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3509683-d403-4748-8a68-9bce5ec8d5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1, m2 = \"gpt-35-turbo\", \"gpt-4-turbo\"\n",
    "v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'\n",
    "\n",
    "prompt1 = \"Given a set of texts with known authors and a query text, determine the author of the query text. \"\n",
    "prompt2 = prompt1+\"Do not consider topic differences. \"\n",
    "prompt3 = prompt1+\"Focus on grammatical styles. \"\n",
    "prompt4 = prompt1+\"Analyze the writing styles of the input texts, disregarding the differences in topic and content. Focus on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \"\n",
    "system_msg = \"\"\"Respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"analysis\": Reasoning behind your answer.\n",
    "  \"answer\": The query text's author ID.\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4501598-a22e-4954-8268-d3eafae00b9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_10 = pd.read_csv(\"llm-aa-res/blog_n10_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "df_10 = pd.read_csv(\"llm-aa-res/email_n10_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "df_10.shape, len(df_10.loc[0, 'potential_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61178ebd-02f3-475e-9d30-7cd2fd176205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_df_10, ls_model_10, ls_method_10 = [], [], []\n",
    "\n",
    "df1_gpt35 = run_aa(df_10, v1, m1, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df2_gpt35 = run_aa(df_10, v2, m1, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df3_gpt35 = run_aa(df_10, v3, m1, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df4_gpt35 = run_aa(df_10, v4, m1, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "\n",
    "df1_gpt4 = run_aa(df_10, v1, m2, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df2_gpt4 = run_aa(df_10, v2, m2, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df3_gpt4 = run_aa(df_10, v3, m2, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df4_gpt4 = run_aa(df_10, v4, m2, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "\n",
    "compare_baseline_mod(df_10, ls_df_10, ls_model_10, ls_method_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30154f10-ebb5-429a-8702-6abe68341fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_20 = pd.read_csv(\"llm-aa-res/blog_n20_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "df_20 = pd.read_csv(\"llm-aa-res/email_n20_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "df_20.shape, len(df_20.loc[0, 'potential_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f925268-bf85-4047-94f2-f61b63347fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ls_df_20, ls_model_20, ls_method_20 = [], [], []\n",
    "\n",
    "df1_gpt35 = run_aa(df_20, v1, m1, prompt1, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "df2_gpt35 = run_aa(df_20, v2, m1, prompt2, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "df3_gpt35 = run_aa(df_20, v3, m1, prompt3, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "df4_gpt35 = run_aa(df_20, v4, m1, prompt4, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "\n",
    "df1_gpt4 = run_aa(df_20, v1, m2, prompt1, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "df2_gpt4 = run_aa(df_20, v2, m2, prompt2, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "df3_gpt4 = run_aa(df_20, v3, m2, prompt3, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "df4_gpt4 = run_aa(df_20, v4, m2, prompt4, system_msg, ls_df_20, ls_model_20, ls_method_20)\n",
    "\n",
    "compare_baseline_mod(df_20, ls_df_20, ls_model_20, ls_method_20, baseline_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a852d-4bfa-4c76-bc10-296b24675f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"llm-aa-res/ls_blog_n10_reps3_eval10.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_df_10, f)\n",
    "with open(\"llm-aa-res/ls_blog_n20_reps3_eval10.pkl\", \"wb\") as f:\n",
    "    pickle.dump(ls_df_20, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a79cc-2333-4f52-9958-68c98ae9ca4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
