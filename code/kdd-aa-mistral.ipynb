{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f5259e8-d7a7-4fe1-b8b3-9577624f759b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Server 4: Authorship Attribution\n",
    "- New prompts\n",
    "- Chunked data\n",
    "- New sampler (Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n",
    "    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de984847-12c4-4673-8de7-a429206e35a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import openai\n",
    "import random\n",
    "import tiktoken\n",
    "import py3langid\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from random import shuffle\n",
    "from ast import literal_eval\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc837a34-8ce5-41be-8953-68c68c7991a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from huggingface_hub import login\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "login(\"YOUR_LOGIN_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dae4bf-5b8c-4c36-8f27-1639fe4c227e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_id = \"TheBloke/Llama-2-13B-chat-GPTQ\"\n",
    "# llm = LLM(model=model_id, quantization='gptq')\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "llm = LLM(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cc9fb-05dd-4c4c-bb7b-52b146ee7eb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string, encoding_name):\n",
    "    encoding = tiktoken.encoding_for_model(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "    \n",
    "\n",
    "def eval_fn(y_test, y_pred, average='weighted', print_flag=True):\n",
    "    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n",
    "    f1 = round(metrics.f1_score(y_test, y_pred, average=average)*100, 2)\n",
    "    recall = round(metrics.recall_score(y_test, y_pred, average=average, zero_division=0)*100, 2)\n",
    "    precision = round(metrics.precision_score(y_test, y_pred, average=average, zero_division=0)*100, 2)\n",
    "    if print_flag:\n",
    "        print(\"Accuracy:\", acc, \"% | Precision:\", precision, \"% | Recall:\", recall, \"% | F1:\", f1, \"%\\n\")   \n",
    "    return acc, precision, recall, f1\n",
    "    \n",
    "\n",
    "def embed_fn(model_name, texts, baseline_type):\n",
    "    if baseline_type == 'bert':\n",
    "        model = AutoModel.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        tokenized_texts = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        embedding = model(tokenized_texts.input_ids.to(model.device), tokenized_texts.attention_mask.to(model.device)).last_hidden_state.mean(dim=1)\n",
    "    elif baseline_type == 'tf-idf':\n",
    "        vectorizer = TfidfVectorizer(max_features=3000, analyzer='char', ngram_range=(4, 4))\n",
    "        embedding = torch.from_numpy(vectorizer.fit_transform(texts).toarray())\n",
    "    elif baseline_type == 'ada':\n",
    "        ada_client = AzureOpenAI(api_key = \"08e99b6c65e84ead8676c505ee4d6f1e\", api_version = \"2023-05-15\", azure_endpoint = \"https://iarpa.openai.azure.com\")\n",
    "        ada_response = ada_client.embeddings.create(input = texts, model = \"test_embedding\")\n",
    "        embedding = torch.Tensor([e.embedding for e in ada_response.data])\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def run_aa_baseline(df_sub, model_name, baseline_type='bert', print_flag=True, eval_average='weighted'):\n",
    "    ls_acc, ls_precision, ls_recall, ls_f1 = [], [], [], []\n",
    "\n",
    "    for i in df_sub.index:\n",
    "        ls_query_text, ls_potential_text = df_sub.loc[i, 'query_text'], df_sub.loc[i, 'potential_text']\n",
    "        embed_query_texts = F.normalize(embed_fn(model_name, ls_query_text, baseline_type)) \n",
    "        embed_potential_texts = F.normalize(embed_fn(model_name, ls_potential_text, baseline_type))\n",
    "        \n",
    "        preds = embed_query_texts @ embed_potential_texts.T\n",
    "        preds = F.softmax(preds, dim=-1)\n",
    "        labels = np.arange(0, len(ls_query_text))\n",
    "\n",
    "        acc, precision, recall, f1 = eval_fn(labels, preds.argmax(-1).numpy(), eval_average, print_flag)\n",
    "        ls_acc.append(acc)\n",
    "        ls_precision.append(precision)\n",
    "        ls_recall.append(recall)\n",
    "        ls_f1.append(f1)\n",
    "\n",
    "    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_precision), 2), round(np.mean(ls_recall), 2), round(np.mean(ls_f1), 2))\n",
    "    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_precision), 2), round(np.std(ls_recall), 2), round(np.std(ls_f1), 2))\n",
    "    return muti_avg, muti_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692dbe0-63a8-4e5e-b6be-c417d09387ca",
   "metadata": {},
   "source": [
    "## Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5455f60-92e5-453c-80af-454f277d4914",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def run_aa(df, method, model_name, prompt_input, system_msg, ls_df, ls_model, ls_method, n_eval=5):\n",
    "    \"\"\"randomly select a subset of query texts\"\"\"\n",
    "    start_time = time.time()\n",
    "    df_res_all = pd.DataFrame()\n",
    "    print(\"\\n++++++++++ \", method, model_name, \" ++++++++++\")\n",
    "\n",
    "    for i in df.index:\n",
    "        ls_reps = []\n",
    "        text_label_map = {}\n",
    "        sampled_queries = []  # select a subset for evaluation (e.g, n_eval out of 10)\n",
    "        ls_query_text, ls_potential_text = df.loc[i, 'query_text'], df.loc[i, 'potential_text']\n",
    "        random.seed(0)\n",
    "        for idx, val in random.sample(list(enumerate(ls_query_text)), n_eval):\n",
    "            text_label_map[val] = idx\n",
    "            sampled_queries.append(val)\n",
    "        # print(text_label_map.values())\n",
    "            \n",
    "        for query_text in sampled_queries:\n",
    "            example_texts = json.dumps(dict(enumerate(ls_potential_text)))\n",
    "            prompt = f\"\"\"<s> [INST] {system_msg} {prompt_input} The input texts are delimited with triple backticks. ```\\n\\nQuery text: {query_text} \\n\\nTexts from potential authors: {example_texts}\\n\\n```[/INST]\"\"\"\n",
    "       \n",
    "            raw_response = llm.generate(prompt, sampling_params)\n",
    "            response_str = raw_response[0].outputs[0].text.strip()\n",
    "            try: \n",
    "                response = json.loads(response_str, strict=False)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"\\n++++++++++ JSONDecodeError ++++++++++\")\n",
    "                response = json.loads(\"{}\")\n",
    "                response['analysis'] = response_str\n",
    "                # ls_possible_ans = [s for s in response_str.split() if s.isdigit()]\n",
    "                ls_possible_ans = re.findall(r'\\d+', response_str)\n",
    "                if len(ls_possible_ans) > 0:\n",
    "                    response['answer'] = ls_possible_ans[-1]\n",
    "                else:\n",
    "                    response['answer'] = -1\n",
    "            print('\\nRaw response:\\n', response['analysis'], '\\nModel prediction:', response['answer'], 'Label:', text_label_map[query_text])\n",
    "                \n",
    "            response[\"query_text\"], response[\"example_texts\"] = query_text, example_texts\n",
    "            response[\"tokens\"] = len(raw_response[0].prompt_token_ids)\n",
    "            response[\"label\"] = text_label_map[query_text]\n",
    "            ls_reps.append(response)\n",
    "\n",
    "        df_reps = pd.DataFrame(ls_reps)\n",
    "        df_reps['answer'] = pd.to_numeric(df_reps['answer'], errors='coerce')\n",
    "        df_reps['answer'] = df_reps['answer'].fillna(-1)\n",
    "        df_res_all = pd.concat([df_res_all, df_reps]).reset_index(drop=True)\n",
    "\n",
    "    ls_df.append(df_res_all)\n",
    "    ls_method.append(method)\n",
    "    ls_model.append(model_name)\n",
    "    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n",
    "    return df_res_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e823f2-cbb1-4d50-9d7d-4523e9e22ab4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_baseline = {'TF-IDF':'TF-IDF', 'BERT':'bert-base-uncased', \n",
    "                 'RoBERTa':'roberta-base', 'ELECTRA':'google/electra-base-discriminator',\n",
    "                 'DeBERTa':'microsoft/deberta-base', 'Ada':'ada v2'}\n",
    "dict_embed_type = {'TF-IDF':'tf-idf', 'BERT':'bert', 'RoBERTa':'bert', \n",
    "                   'ELECTRA':'bert', 'DeBERTa':'bert', 'Ada':'ada'}\n",
    "\n",
    "def compare_baseline_mod(df_sub, ls_df, ls_model, ls_method, n_eval=5, std_flag=False):\n",
    "    ls_res_avg, ls_res_std = [], []\n",
    "\n",
    "    for key, val in list(dict_baseline.items())[:1]:\n",
    "        muti_avg, muti_std = run_aa_baseline(df_sub, val, dict_embed_type[key], print_flag=False)\n",
    "        ls_res_avg.append((key, val)+muti_avg+(0,))\n",
    "        ls_res_std.append((key, val)+muti_std+(0,))\n",
    "\n",
    "    for i, df_tmp in enumerate(ls_df):\n",
    "        muti_avg, muti_std = eval_all_fn(df_tmp, n_eval)\n",
    "        answer_tmp = df_tmp.copy()\n",
    "        \n",
    "        ls_res_avg.append((ls_method[i], ls_model[i])+muti_avg+(abs(answer_tmp[answer_tmp.answer==-1]['answer'].astype('int').sum()),))\n",
    "        ls_res_std.append((ls_method[i], ls_model[i])+muti_std+(None,))\n",
    "    \n",
    "    res_avg = pd.DataFrame(ls_res_avg, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Unsure'])\n",
    "    res_std = pd.DataFrame(ls_res_std, columns=['Prompt', 'Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'Unsure'])\n",
    "    if std_flag:\n",
    "        return res_avg, res_std\n",
    "    else:\n",
    "        return res_avg\n",
    "\n",
    "\n",
    "def eval_all_fn(df_res_all, n_eval):\n",
    "    \"\"\"evaluate the entire df of multiple repetitions, take avg of each rep. \n",
    "    The null or -1 answers are counted as false\n",
    "    Make sure n_eval is same in run_aa()\"\"\"\n",
    "    ls_acc, ls_precision, ls_recall, ls_f1 = [], [], [], []\n",
    "    for i in range(0, len(df_res_all.index), n_eval):\n",
    "        df_reps = df_res_all[i: i+n_eval]\n",
    "        acc, precision, recall, f1 = eval_fn(df_reps[\"label\"], df_reps[\"answer\"], average='weighted', print_flag=False)\n",
    "        ls_acc.append(acc)\n",
    "        ls_precision.append(precision)\n",
    "        ls_recall.append(recall)\n",
    "        ls_f1.append(f1)\n",
    "        \n",
    "    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_precision), 2), round(np.mean(ls_recall), 2), round(np.mean(ls_f1), 2))\n",
    "    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_precision), 2), round(np.std(ls_recall), 2), round(np.std(ls_f1), 2))\n",
    "    return muti_avg, muti_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df22c17-e817-42c9-95d9-53fc58e5fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(temperature=0.0, top_p=1.0, max_tokens=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3509683-d403-4748-8a68-9bce5ec8d5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1, m2 = \"GPT-3.5 Turbo\", \"GPT-4 Turbo\"\n",
    "v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'\n",
    "\n",
    "prompt1 = \"Given a set of texts with known authors and a query text, determine the author of the query text. \"\n",
    "prompt2 = prompt1+\"Do not consider topic differences. \"\n",
    "prompt3 = prompt1+\"Focus on grammatical styles. \"\n",
    "prompt4 = prompt1+\"Analyze the writing styles of the input texts, disregarding the differences in topic and content. Focus on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655845ed-30df-45c7-9597-a36f7c6247a7",
   "metadata": {},
   "source": [
    "## Blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b68892-f27f-4188-b1a5-89228c2adcca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_10 = pd.read_csv(\"llm-aa-res/blog_n10_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "df_10.shape, len(df_10.loc[0, 'potential_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad2be2-a4b3-4187-9e49-d3102092baf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system_msg = \"\"\"Respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"answer\": The query text's author ID.\n",
    "  \"analysis\": Reasoning behind your answer.\n",
    "}\"\"\"\n",
    "ls_df_10, ls_model_10, ls_method_10 = [], [], []\n",
    "df1 = run_aa(df_10, v1, model_id, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df2 = run_aa(df_10, v2, model_id, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df3 = run_aa(df_10, v3, model_id, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df4 = run_aa(df_10, v4, model_id, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "compare_baseline_mod(df_10, ls_df_10, ls_model_10, ls_method_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ea466-0377-40a9-89b3-738b1426e794",
   "metadata": {},
   "source": [
    "## Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d9d94-6d15-45dd-b7cf-b85feb323847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_10 = pd.read_csv(\"llm-aa-res/email_n10_reps3.csv\", converters={\"query_text\": literal_eval, \"potential_text\": literal_eval})\n",
    "print(df_10.shape, len(df_10.loc[0, 'potential_text']))\n",
    "system_msg = \"\"\"Respond with a JSON object including two key elements:\n",
    "{\n",
    "  \"analysis\": Authorship analysis.\n",
    "  \"answer\": The query text's author ID as an integer.\n",
    "}\"\"\"\n",
    "ls_df_10, ls_model_10, ls_method_10 = [], [], []\n",
    "df1 = run_aa(df_10, v1, model_id, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df2 = run_aa(df_10, v2, model_id, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df3 = run_aa(df_10, v3, model_id, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "df4 = run_aa(df_10, v4, model_id, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n",
    "compare_baseline_mod(df_10, ls_df_10, ls_model_10, ls_method_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79373d32-38ed-4fa5-8dae-4489eccf969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
